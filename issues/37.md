**Task:**
Integrate a caching layer into the project using [Valkey](https://valkey.io/), a Redis-compatible cache server implementation.

**Use Case:**
Improve the `/temperature` endpoint performance by caching the aggregated temperature data. Instead of making direct API calls to openSenseMap on every request, the endpoint will serve cached data that is refreshed once per minute.

**Implementation Details:**
- Deploy and configure Valkey as the caching backend.
- Implement caching for the `/temperature` endpoint:
  - Cache key: `temperature:latest`
  - Cache TTL (time-to-live): 60 seconds
  - Cache value: JSON object containing average temperature, status, and timestamp.
- Introduce a background job that fetches temperature data from openSenseMap **once per minute** and updates the Valkey cache.
- The `/temperature` endpoint reads from Valkey cache instead of making direct API calls.
- Implement proper fallback mechanism if Valkey is unavailable (fallback to direct API calls with appropriate logging).

**Acceptance Criteria:**
- Valkey is deployed and configured as the caching backend.
- The `/temperature` endpoint serves data from Valkey cache.
- Background job refreshes cache every 60 seconds.
- Proper error handling and fallback if cache is unavailable.
- Cache hit/miss metrics are tracked (integrate with Prometheus metrics).
- Update configuration and documentation to reflect Valkey usage, including setup instructions.

**Benefits:**
- Reduced latency for `/temperature` endpoint (cache read vs. API call).
- Lower load on openSenseMap API (1 request per minute vs. per user request).
- Better resilience (cached data available even during brief API outages).
- Improved user experience with consistent response times.

**Notes:**
- Valkey is a drop-in, open-source Redis replacement and supports most Redis tooling.
- Focus on performance, reliability, and clear separation of cache concerns.
- Consider adding cache warming on application startup.